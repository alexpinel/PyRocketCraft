"""
  Train a neural network to imitate the model predictive control
  outputs.

  (c) Jan Zwiener (jan@zwiener.org)
"""

import time
from datetime import datetime
import json
import numpy as np
import torch
from torch import nn
from torch import optim
from torch.utils.data import TensorDataset, DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter

from nnpolicynetwork import NNPolicyNetwork

def train_and_evaluate():
    """
      (!) This requires as input training data in the expert_data.json file.

      The output is a network file "torch_nn_mpc-rocket-vX.pth"
      that is used by nnpolicy.py (NNPolicy).
    """

    print("pytorch version: ", end="")
    print(torch.__version__)

    INPUT_FILE = "expert_data.json"
    OUTPUT_FILE = "torch_nn_mpc-rocket-v3.pth"
    BATCH_SIZE = 32
    LEARNING_RATE = 0.0005
    EPOCHS = 100
    SEED = 15
    VAL_PERCENTAGE = 0.1  # 10% for validation

    print("Loading data from file: %s" % INPUT_FILE)
    # Load training data, e.g. generated by the MPC controller
    with open(INPUT_FILE, "r", encoding="utf-8") as file:
        data = json.load(file)

    torch.manual_seed(SEED)

    print("Found %i state/action pairs" % (len(data)))
    # Assuming each entry in data is a dictionary with 'obs' and 'acts' keys
    observations = np.array([item['obs'] for item in data])
    actions = np.array([item['acts'] for item in data])
    # Predicted state vectors can also be stored in the JSON file
    # predictedX = np.array([item['predictedX'] for item in data])

    # select device
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print("pytorch running training on gpu")
        print("    cuda device: %s" % (torch.cuda.get_device_name()))
        print("    number of cuda devices: %i" % (torch.cuda.device_count()))
        torch.cuda.manual_seed(SEED)
        torch.cuda.manual_seed_all(SEED)
        torch.backends.cudnn.deterministic = True
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("pytorch Apple silicon (mps) available")
        print("Hint: manually check if cpu performance is potentially better")
    else:
        device = torch.device("cpu")
        print("pytorch running training on cpu")

    tic = time.time()

    # Setting up pytorch
    observations = torch.tensor(observations, dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.float32)
    dataset = TensorDataset(observations, actions)
    # data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    input_size = observations.shape[1]
    output_size = actions.shape[1]
    model = NNPolicyNetwork(input_size, output_size).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()

    # Split data into training and validation sets
    total_size = len(dataset)
    val_size = int(total_size * VAL_PERCENTAGE)
    train_size = total_size - val_size

    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

    print("Learning rate: %.4f, batch size=%i" % (LEARNING_RATE, BATCH_SIZE))
    print("Input vector size: %i" % input_size)
    print("Output vector size: %i" % output_size)
    print("Epochs: %i" % EPOCHS)

    current_time = datetime.now().strftime('%Y%m%d-%H%M%S')
    log_dir = f"runs/run_{current_time}_lr_{LEARNING_RATE}_bs_{BATCH_SIZE}"
    writer = SummaryWriter(log_dir=log_dir) # TensorBoard / torchvision output

    # Training loop
    for epoch in range(EPOCHS):
        # Training phase
        model.train()
        train_loss = 0.0
        for inputs, targets in train_loader:
            # Move data to the device (e.g. GPU)
            inputs, targets = inputs.to(device), targets.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            # TensorBoard:
            writer.add_scalar("Loss/train", loss, epoch)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
        avg_train_loss = train_loss / len(train_loader)

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                val_loss += loss.item()
                # TensorBoard:
                writer.add_scalar("Loss/val", loss, epoch)
        avg_val_loss = val_loss / len(val_loader)

        print(f"Epoch [{epoch+1}/{EPOCHS}], ", end="")
        print(f"Average Train Loss: {avg_train_loss:.4f}, ", end="")
        print(f"Average Validation Loss: {avg_val_loss:.4f}, ", end="")
        print("")

    toc = time.time()
    print("Print training time: %.0f seconds (%.3f s / epoch)" %
          (toc-tic, (toc-tic)/EPOCHS))

    writer.flush()
    writer.close() # flush required?

    # Save the trained model to disk
    model.to('cpu')
    torch.save(model.state_dict(), OUTPUT_FILE)

if __name__ == '__main__':
    train_and_evaluate()
