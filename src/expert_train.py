"""
  Train a neural network to imitate the model predictive control
  outputs.

  (c) Jan Zwiener (jan@zwiener.org)
"""

import json
import numpy as np
import torch
from torch import nn
from torch import optim
from torch.utils.data import TensorDataset, DataLoader

from nnpolicynetwork import NNPolicyNetwork
from expert_validate import expert_validate

def train_and_evaluate():
    """
      (!) This requires as input training data in the expert_data.json file.

      The output is a network file "torch_nn_mpc-rocket-vX.pth"
      that is used by nnpolicy.py (NNPolicy).
    """

    INPUT_FILE = "expert_data.json"
    OUTPUT_FILE = "torch_nn_mpc-rocket-v3.pth"
    BATCH_SIZE = 128
    LEARNING_RATE = 0.001

    print("Loading data from file: %s" % INPUT_FILE)
    # Load training data, e.g. generated by the MPC controller
    with open(INPUT_FILE, "r", encoding="utf-8") as file:
        data = json.load(file)

    print("Found %i state/action pairs" % (len(data)))
    # Assuming each entry in data is a dictionary with 'obs' and 'acts' keys
    observations = np.array([item['obs'] for item in data])
    actions = np.array([item['acts'] for item in data])
    # Predicted state vectors can also be stored in the JSON file
    # predictedX = np.array([item['predictedX'] for item in data])

    # try to use CUDA if available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"PyTorch: using {device}")

    # Setting up pytorch
    observations = torch.tensor(observations, dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.float32)
    dataset = TensorDataset(observations, actions)
    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    input_size = observations.shape[1]
    output_size = actions.shape[1]
    model = NNPolicyNetwork(input_size, output_size).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()
    print("Learning rate: %.4f, batch size=%i" % (LEARNING_RATE, BATCH_SIZE))
    print("Input vector size: %i" % input_size)
    print("Output vector size: %i" % output_size)

    # Training loop
    num_epochs = 100
    for epoch in range(num_epochs):
        model.train()  # Set the model to training mode
        running_loss = 0.0

        for inputs, targets in data_loader:
            # Move data to the device (e.g. GPU)
            inputs, targets = inputs.to(device), targets.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        # Compute the average loss for this epoch
        avg_loss = running_loss / len(data_loader)

        print(f"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}")

    # Save the trained model to disk
    model.to('cpu')
    torch.save(model.state_dict(), OUTPUT_FILE)

if __name__ == '__main__':
    train_and_evaluate()
