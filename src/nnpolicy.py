# (c) Jan Zwiener (jan@zwiener.org)
#
# Imitate the MPCPolicy controller with a neural network
#
# This controller is very simple: load the trained network
# data (generated by expert_train.py) and just run
# inference on the input to the step() function, i.e.
# based on the supplied state, calculate a control input (u)
# vector / action vector.

import numpy as np
import torch
import torch.nn as nn

from basecontrol import BaseControl
from nnpolicynetwork import NNPolicyNetwork
from geodetic_toolbox import quat_to_matrix, quat_invert

def augment_state(state):
    """
        Helper function to prepare the state vector for
        the neural network.

        input: state vector
        output: augumented state vector
    """

    augmented_state = state.copy()

    # state has body to nav. transformation
    # q_nav_to_body = quat_invert(augmented_state[0:4])
    # R = quat_to_matrix(q_nav_to_body)

    # delta to setpoint
    # reference_pos = np.array([0.0, 0.0, 0.0])

    # pos_n = augmented_state[7:10]
    # vel_n = augmented_state[10:13]
    # FIXME: state config is hardcoded here
    # also the setpoint / reference position is hardcoded here
    # Getting from simrocketenv state_cfg would work,
    # but is a bit of heavy lifting. maybe just add a
    # test case?

    # pos_b = R@(reference_pos - pos_n)
    # vel_b = R@vel_n

    # augmented_state[7:10] = pos_b
    # augmented_state[10:13] = vel_b
    # augmented_state[10:13] = vel_b

    # augmented_state[13] = 0.0
    # augmented_state[14] = 0.0
    # augmented_state[15] = 0.0

    return augmented_state

class NNPolicy(BaseControl):
    def __init__(self, network_file="torch_nn_mpc-rocket-v2.pth"):
        super().__init__()

        print("pytorch version: ", end="")
        print(torch.__version__)

        input_size = 16
        output_size = 5
        self.model = NNPolicyNetwork(input_size, output_size)
        self.model.load_state_dict(torch.load(network_file))

        # Default to CPU for inference, as the model is quite small

        # if torch.cuda.is_available():
        #     device = torch.device("cuda")
        #     print("pytorch running inference on gpu")
        #     print("    cuda device: %s" % (torch.cuda.get_device_name()))
        #     print("    number of cuda devices: %i" % (torch.cuda.device_count()))
        # elif torch.backends.mps.is_available():
        #     device = torch.device("mps")
        #     print("pytorch running inference on Apple silicon")
        # else:
        #     device = torch.device("cpu")
        #     print("pytorch running inference on cpu")
        device = torch.device("cpu")
        print("pytorch running inference on cpu")

        self.model = self.model.to(device)
        self.model.eval()  # Set the model to inference mode


    def get_name(self):
        return "NN"

    def next(self, observation):
        """
            Get a control input (u) / action vector for
            a supplied state vector (observation).
        """

        state = augment_state(observation)

        device = next(self.model.parameters()).device
        state_tensor = torch.tensor(state, dtype=torch.float32).to(device)
        state_tensor = state_tensor.unsqueeze(0)

        with torch.no_grad():  # Disables gradient calculation
            action_pred = self.model(state_tensor)

        # If the model is on GPU, move the result back to CPU for numpy
        # conversion
        action = action_pred.cpu().numpy()
        predictedX = np.ndarray((0, 0))

        return action.ravel(), predictedX
